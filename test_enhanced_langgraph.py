"""
Test script for Enhanced Mock Extractor with LangGraph Framework
Verifies the complete multi-agent LangGraph workflow
"""

import sys
import os

# Add the src directory to the path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def test_langgraph_workflow():
    """Test the complete LangGraph multi-agent workflow"""
    print("üß™ Testing Enhanced Mock Extractor with LangGraph Framework...")
    
    try:
        from src.core.enhanced_mock_extractor import EnhancedMockCoreConceptExtractor, ValidationFeedback, SeedKeywords
        print("‚úÖ Successfully imported EnhancedMockCoreConceptExtractor")
    except ImportError as e:
        print(f"‚ùå Failed to import enhanced mock extractor: {e}")
        print("üí° Make sure you have installed the required dependencies:")
        print("   pip install langgraph pydantic")
        return False
    
    # Create extractor instance with LangGraph
    print("\nüîß Creating Enhanced Mock Extractor with LangGraph...")
    extractor = EnhancedMockCoreConceptExtractor(
        model_name="mock-llm-langgraph",
        use_checkpointer=False  # Set to True to test checkpointing
    )
    
    # Verify LangGraph components
    print("üìä Verifying LangGraph components...")
    if hasattr(extractor, 'graph'):
        print("  ‚úÖ LangGraph StateGraph exists")
        print(f"  üìã Graph type: {type(extractor.graph)}")
    else:
        print("  ‚ùå LangGraph StateGraph missing")
        return False
    
    # Test input
    test_input = """
    I want to develop a smart home automation system that uses AI and IoT sensors 
    to automatically control lighting, temperature, and security based on occupancy 
    patterns and user preferences. The system should learn from user behavior and 
    optimize energy consumption while maintaining comfort and security.
    """
    
    print(f"\nüìù Input text: {test_input[:100]}...")
    
    try:
        print("\nüîÑ Running LangGraph multi-agent workflow...")
        print("=" * 60)
        
        # Run extraction through LangGraph
        results = extractor.extract_keywords(test_input)
        
        print("=" * 60)
        print("‚úÖ LangGraph workflow completed successfully!")
        
        # Verify results structure
        expected_keys = [
            'input_text', 'problem', 'technical', 'summary_text', 'ipcs',
            'concept_matrix', 'seed_keywords', 'validation_feedback', 
            'final_keywords', 'queries', 'final_url'
        ]
        
        print("\nüìä Checking LangGraph results structure...")
        missing_keys = []
        for key in expected_keys:
            if key in results:
                print(f"  ‚úÖ {key}: {type(results[key])}")
            else:
                print(f"  ‚ùå Missing key: {key}")
                missing_keys.append(key)
        
        if missing_keys:
            print(f"\n‚ö†Ô∏è Missing keys: {missing_keys}")
            return False
        
        # Display key results from LangGraph workflow
        print("\nüéØ LangGraph Multi-Agent Results:")
        
        if results.get('concept_matrix'):
            cm = results['concept_matrix']
            print(f"  üéØ Problem/Purpose: {cm.problem_purpose[:100]}...")
            print(f"  üîß Object/System: {cm.object_system[:100]}...")
            print(f"  üåç Environment/Field: {cm.environment_field[:100]}...")
        
        if results.get('seed_keywords'):
            sk = results['seed_keywords']
            print(f"  üîë Problem Keywords: {sk.problem_purpose}")
            print(f"  üîë Object Keywords: {sk.object_system}")
            print(f"  üîë Environment Keywords: {sk.environment_field}")
        
        if results.get('final_keywords'):
            print(f"  üîç Final Keywords: {len(results['final_keywords'])} categories")
            for key, synonyms in list(results['final_keywords'].items())[:3]:  # Show first 3
                print(f"    ‚Ä¢ {key}: {synonyms}")
        
        if results.get('queries'):
            print(f"  üîç Generated Queries: {len(results['queries'].queries)} queries")
            for i, query in enumerate(results['queries'].queries[:2], 1):  # Show first 2
                print(f"    {i}. {query}")
        
        if results.get('final_url'):
            print(f"  üîó Found URLs: {len(results['final_url'])} patents")
            for i, url_info in enumerate(results['final_url'][:2], 1):  # Show first 2
                print(f"    {i}. {url_info['url']} (scores: {url_info['user_scenario']:.3f}, {url_info['user_problem']:.3f})")
        
        print("\nüéâ Enhanced Mock Extractor with LangGraph test completed successfully!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå LangGraph workflow failed with error: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_langgraph_with_checkpointing():
    """Test LangGraph workflow with checkpointing enabled"""
    print("\nüß™ Testing LangGraph with Checkpointing...")
    
    try:
        from src.core.enhanced_mock_extractor import EnhancedMockCoreConceptExtractor
        
        # Create extractor with checkpointing enabled
        extractor = EnhancedMockCoreConceptExtractor(
            model_name="mock-llm-checkpoint",
            use_checkpointer=True  # Enable checkpointing
        )
        
        test_input = "Smart IoT home automation with AI learning capabilities"
        
        print("üîÑ Running LangGraph workflow with checkpointing...")
        results = extractor.extract_keywords(test_input)
        
        if results and len(results) > 5:  # Basic check for results
            print("‚úÖ LangGraph checkpointing test passed!")
            return True
        else:
            print("‚ùå LangGraph checkpointing test failed - insufficient results")
            return False
            
    except Exception as e:
        print(f"‚ùå LangGraph checkpointing test failed: {str(e)}")
        return False

def test_custom_evaluation_handler():
    """Test LangGraph with custom evaluation handler"""
    print("\nüß™ Testing LangGraph with Custom Evaluation Handler...")
    
    try:
        from src.core.enhanced_mock_extractor import EnhancedMockCoreConceptExtractor, ValidationFeedback, SeedKeywords
        
        # Custom evaluation handler for testing
        def mock_ui_evaluation(state):
            """Mock UI evaluation handler that simulates user interaction"""
            print("  üì± Custom LangGraph evaluation handler called")
            print(f"  üìä LangGraph state keys: {list(state.keys())}")
            
            # Simulate user editing keywords
            if state.get('seed_keywords'):
                edited_keywords = SeedKeywords(
                    problem_purpose=["smart_automation", "ai_learning"],
                    object_system=["iot_sensors", "home_system"],
                    environment_field=["smart_home", "residential"]
                )
                return {"validation_feedback": ValidationFeedback(action="edit", edited_keywords=edited_keywords)}
            
            return {"validation_feedback": ValidationFeedback(action="approve")}
        
        # Create extractor with custom handler
        extractor = EnhancedMockCoreConceptExtractor(
            custom_evaluation_handler=mock_ui_evaluation
        )
        
        test_input = "AI-powered smart home automation system"
        
        print("üîÑ Running LangGraph with custom evaluation handler...")
        results = extractor.extract_keywords(test_input)
        
        # Check if custom evaluation was used and edited keywords were applied
        if (results.get('validation_feedback') and 
            results['validation_feedback'].action == "edit" and
            results.get('seed_keywords') and
            "smart_automation" in results['seed_keywords'].problem_purpose):
            
            print("‚úÖ LangGraph custom evaluation handler test passed!")
            print("‚úÖ Edited keywords were correctly applied in LangGraph workflow")
            return True
        else:
            print("‚ùå LangGraph custom evaluation handler test failed")
            return False
            
    except Exception as e:
        print(f"‚ùå LangGraph custom evaluation test failed: {str(e)}")
        return False

def test_langgraph_architecture():
    """Test that LangGraph architecture matches original extractor.py"""
    print("\nüß™ Testing LangGraph Architecture Consistency...")
    
    try:
        from src.core.enhanced_mock_extractor import EnhancedMockCoreConceptExtractor
        
        extractor = EnhancedMockCoreConceptExtractor()
        
        # Check LangGraph-specific components
        langgraph_components = ['graph']
        for component in langgraph_components:
            if hasattr(extractor, component):
                print(f"  ‚úÖ LangGraph {component} exists")
            else:
                print(f"  ‚ùå Missing LangGraph {component}")
                return False
        
        # Check that all workflow methods exist (should be LangGraph nodes)
        workflow_methods = [
            'input_normalization', 'step0', 'step1_concept_extraction',
            'step2_keyword_generation', 'step3_human_evaluation', 
            'manual_editing', 'gen_key', 'summary_prompt_and_parser',
            'call_ipcs_api', 'genQuery', 'genUrl', 'evalUrl'
        ]
        
        for method in workflow_methods:
            if hasattr(extractor, method) and callable(getattr(extractor, method)):
                print(f"  ‚úÖ LangGraph node method: {method}")
            else:
                print(f"  ‚ùå Missing LangGraph node method: {method}")
                return False
        
        # Check LangGraph-specific methods
        langgraph_methods = ['_build_graph', '_get_human_action']
        for method in langgraph_methods:
            if hasattr(extractor, method) and callable(getattr(extractor, method)):
                print(f"  ‚úÖ LangGraph method: {method}")
            else:
                print(f"  ‚ùå Missing LangGraph method: {method}")
                return False
        
        print("‚úÖ LangGraph architecture consistency test passed!")
        return True
        
    except Exception as e:
        print(f"‚ùå LangGraph architecture test failed: {str(e)}")
        return False

if __name__ == "__main__":
    print("üöÄ Running Enhanced Mock Extractor LangGraph Tests\n")
    
    # Run all tests
    basic_test = test_langgraph_workflow()
    checkpoint_test = test_langgraph_with_checkpointing()
    handler_test = test_custom_evaluation_handler()
    architecture_test = test_langgraph_architecture()
    
    print(f"\nüìä LangGraph Test Summary:")
    print(f"  Basic LangGraph Workflow: {'‚úÖ PASS' if basic_test else '‚ùå FAIL'}")
    print(f"  LangGraph Checkpointing: {'‚úÖ PASS' if checkpoint_test else '‚ùå FAIL'}")
    print(f"  Custom Handler Integration: {'‚úÖ PASS' if handler_test else '‚ùå FAIL'}")
    print(f"  LangGraph Architecture: {'‚úÖ PASS' if architecture_test else '‚ùå FAIL'}")
    
    if all([basic_test, checkpoint_test, handler_test, architecture_test]):
        print("\nüéâ All LangGraph tests passed! Enhanced Mock Extractor is ready for use.")
        print("üí° The multi-agent LangGraph architecture is working correctly with mock data.")
        sys.exit(0)
    else:
        print("\n‚ö†Ô∏è Some LangGraph tests failed. Please check the implementation.")
        sys.exit(1)
