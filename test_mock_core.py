#!/usr/bin/env python3
"""
Test script for the core Mock Patent AI Agent functionality
Tests only the mock extractor without Streamlit dependencies
"""

import sys
import os
import json

def test_mock_imports():
    """Test that mock components can be imported"""
    print("ğŸ§ª Testing mock system imports...")
    
    try:
        from src.core.mock_extractor import MockCoreConceptExtractor, ValidationFeedback, SeedKeywords, MockLLM
        print("âœ… Mock extractor imports successful")
        return True
    except Exception as e:
        print(f"âŒ Mock extractor import failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_mock_llm():
    """Test that mock LLM produces responses"""
    print("\nğŸ§ª Testing mock LLM responses...")
    
    try:
        from src.core.mock_extractor import MockLLM
        
        llm = MockLLM()
        
        # Test different types of prompts
        test_prompts = [
            ("normalization", "normalization prompt test"),
            ("concept matrix", "concept matrix prompt test"), 
            ("keywords", "seed keywords prompt test"),
            ("summary", "summary prompt test"),
            ("queries", "queries prompt test"),
            ("synonyms", "synonyms prompt test")
        ]
        
        for prompt_type, prompt in test_prompts:
            response = llm.invoke(prompt)
            if response and len(response) > 10:
                print(f"âœ… {prompt_type.title()} response generated ({len(response)} chars)")
                
                # Try to parse JSON responses
                try:
                    cleaned_response = response.strip()
                    json.loads(cleaned_response)
                    print(f"  âœ… Response is valid JSON")
                except:
                    print(f"  âš ï¸  Response is not JSON (might be intentional)")
            else:
                print(f"âŒ {prompt_type.title()} response too short or empty")
                return False
                
        return True
        
    except Exception as e:
        print(f"âŒ Mock LLM test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_validation_models():
    """Test validation models work correctly"""
    print("\nğŸ§ª Testing validation models...")
    
    try:
        from src.core.mock_extractor import ValidationFeedback, SeedKeywords
        
        # Test ValidationFeedback
        feedback1 = ValidationFeedback(action="approve")
        print(f"âœ… Approve feedback: {feedback1.action}")
        
        feedback2 = ValidationFeedback(action="reject", feedback="Test feedback")
        print(f"âœ… Reject feedback: {feedback2.action}, {feedback2.feedback}")
        
        # Test SeedKeywords
        keywords = SeedKeywords(
            problem_purpose=["water", "optimization"],
            object_system=["IoT", "sensors"], 
            environment_field=["agriculture", "farming"]
        )
        print(f"âœ… SeedKeywords created with {len(keywords.problem_purpose)} problem keywords")
        
        feedback3 = ValidationFeedback(action="edit", edited_keywords=keywords)
        print(f"âœ… Edit feedback with {len(feedback3.edited_keywords.object_system)} object keywords")
        
        return True
        
    except Exception as e:
        print(f"âŒ Validation models test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_mock_extractor():
    """Test that mock extractor can run a complete workflow"""
    print("\nğŸ§ª Testing mock extractor workflow...")
    
    try:
        from src.core.mock_extractor import MockCoreConceptExtractor, ValidationFeedback
        
        # Create extractor with auto-approval handler
        def auto_approve_handler(state):
            print("  ğŸ¤– Auto-approval handler called")
            return {"validation_feedback": ValidationFeedback(action="approve")}
        
        extractor = MockCoreConceptExtractor(custom_evaluation_handler=auto_approve_handler)
        print("âœ… Mock extractor created")
        
        # Test input
        test_input = """
        Smart Irrigation System with IoT Sensors for precision agriculture.
        Problem: Traditional irrigation wastes water and lacks real-time monitoring.
        Solution: IoT sensors monitor soil moisture and control irrigation automatically.
        Technical approach: Wireless sensor network with machine learning algorithms.
        """
        
        print("ğŸ”„ Running mock extraction workflow...")
        results = extractor.extract_keywords(test_input)
        print("âœ… Workflow completed")
        
        # Validate results structure
        required_keys = ['concept_matrix', 'seed_keywords', 'final_keywords', 'queries', 'final_url']
        for key in required_keys:
            if key in results:
                print(f"âœ… {key} present in results")
            else:
                print(f"âŒ {key} missing from results")
                return False
        
        # Test specific result content
        if hasattr(results['concept_matrix'], 'problem_purpose'):
            print(f"âœ… Concept matrix: {results['concept_matrix'].problem_purpose[:50]}...")
        else:
            print("âŒ Concept matrix structure invalid")
            return False
            
        if hasattr(results['seed_keywords'], 'problem_purpose'):
            keywords = results['seed_keywords'].problem_purpose
            print(f"âœ… Seed keywords ({len(keywords)}): {keywords}")
        else:
            print("âŒ Seed keywords structure invalid")
            return False
        
        if isinstance(results['final_keywords'], dict) and len(results['final_keywords']) > 0:
            print(f"âœ… Final keywords generated for {len(results['final_keywords'])} terms")
        else:
            print("âŒ Final keywords not properly generated")
            return False
            
        if hasattr(results['queries'], 'queries') and len(results['queries'].queries) > 0:
            print(f"âœ… {len(results['queries'].queries)} search queries generated")
        else:
            print("âŒ Search queries not properly generated")
            return False
            
        if isinstance(results['final_url'], list) and len(results['final_url']) > 0:
            print(f"âœ… {len(results['final_url'])} patent URLs found")
        else:
            print("âŒ Patent URLs not properly generated")
            return False
        
        print("âœ… Mock extractor workflow completed successfully")
        return True
        
    except Exception as e:
        print(f"âŒ Mock extractor test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_rejection_workflow():
    """Test rejection and retry workflow"""
    print("\nğŸ§ª Testing rejection workflow...")
    
    try:
        from src.core.mock_extractor import MockCoreConceptExtractor, ValidationFeedback
        
        call_count = 0
        
        def rejection_handler(state):
            nonlocal call_count
            call_count += 1
            
            if call_count == 1:
                print("  ğŸš« First call - rejecting")
                return {"validation_feedback": ValidationFeedback(action="reject", feedback="Test rejection")}
            else:
                print("  âœ… Second call - approving")
                return {"validation_feedback": ValidationFeedback(action="approve")}
        
        extractor = MockCoreConceptExtractor(custom_evaluation_handler=rejection_handler)
        
        test_input = "Test patent idea for rejection workflow"
        
        print("ğŸ”„ Running rejection workflow test...")
        results = extractor.extract_keywords(test_input)
        
        if call_count == 2:
            print("âœ… Rejection workflow worked - handler called twice")
            return True
        else:
            print(f"âŒ Rejection workflow failed - handler called {call_count} times")
            return False
        
    except Exception as e:
        print(f"âŒ Rejection workflow test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run all core mock system tests"""
    print("ğŸ­ Patent AI Agent - Core Mock System Test")
    print("=" * 60)
    print("ğŸ§ª Testing without Streamlit dependencies")
    print("=" * 60)
    
    tests = [
        test_mock_imports,
        test_validation_models,
        test_mock_llm,
        test_mock_extractor,
        test_rejection_workflow
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
        else:
            print("âŒ Test failed, stopping...")
            break
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š Core Mock System Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("âœ… All core tests passed! Mock system is working correctly.")
        print("\nğŸ­ Core mock functionality validated:")
        print("   âœ… Mock LLM responses")
        print("   âœ… Complete extraction workflow")
        print("   âœ… Human evaluation simulation") 
        print("   âœ… Rejection/retry logic")
        print("   âœ… Data model validation")
        print("\nğŸš€ To run the full demo (requires Streamlit):")
        print("   pip install streamlit pandas")
        print("   python run_demo.py")
    else:
        print("âŒ Some core tests failed. Please fix the issues.")
        sys.exit(1)

if __name__ == "__main__":
    main()
